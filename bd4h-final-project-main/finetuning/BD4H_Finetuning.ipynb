{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLwyKGPwmrSg"
      },
      "source": [
        "# FineTuning Data and Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-8wjaHFmwI_"
      },
      "outputs": [],
      "source": [
        "# importing packages\n",
        "#!pip install accelerate -U\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import json\n",
        "import argparse\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import TrainingArguments,BartTokenizer, BartConfig,BartForConditionalGeneration,Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List,Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNotBu2N3Lsb",
        "outputId": "fc671d27-7585-400e-c6ac-b873a10b0daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content\n",
            "Mounted at /content/gdrive\n",
            "Working Directory: /content/gdrive/My Drive/Georgia Tec/BD4H/Entity_Linking\n"
          ]
        }
      ],
      "source": [
        "#connecting to the data file\n",
        "'''\n",
        "!pip install gdown\n",
        "!gdown https://drive.google.com/drive/folders/1CPEDAJ2Kezx6U2zGQhlmVS3Je3cW8h1e --folder'''\n",
        "\n",
        "!set CUDA_VISIBLE_DEVICES=0\n",
        "from google.colab import drive\n",
        "# View current working directory\n",
        "print(\"Current Working Directory:\", os.getcwd())\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "# Change working directory to your file position\n",
        "path = \"/content/gdrive/My Drive/Georgia Tec/BD4H/Entity_Linking\"\n",
        "os.chdir(path)\n",
        "# Confirm the change\n",
        "print(\"Working Directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xddTi-w4k3uu"
      },
      "source": [
        "Prefix Tree Trie Class - Class to hold the prefix tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBohnsB50qGp"
      },
      "outputs": [],
      "source": [
        "#class definitions\n",
        "\n",
        "# creating prefix tree\n",
        "#NOTE: The Trie data structure is reused from the provided code.\n",
        "class Trie(object):\n",
        "    def __init__(self, sequences: List[List[int]] = []):\n",
        "        self.trie_dict = {}\n",
        "        self.len = 0\n",
        "        if sequences:\n",
        "            for sequence in sequences:\n",
        "                Trie._add_to_trie(sequence, self.trie_dict)\n",
        "                self.len += 1\n",
        "\n",
        "        self.append_trie = None\n",
        "        self.bos_token_id = None\n",
        "\n",
        "    def append(self, trie, bos_token_id):\n",
        "        self.append_trie = trie\n",
        "        self.bos_token_id = bos_token_id\n",
        "\n",
        "    def add(self, sequence: List[int]):\n",
        "        Trie._add_to_trie(sequence, self.trie_dict)\n",
        "        self.len += 1\n",
        "\n",
        "    def get(self, prefix_sequence: List[int]):\n",
        "        return Trie._get_from_trie(\n",
        "            prefix_sequence, self.trie_dict, self.append_trie, self.bos_token_id\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def load_from_dict(trie_dict):\n",
        "        trie = Trie()\n",
        "        trie.trie_dict = trie_dict\n",
        "        trie.len = sum(1 for _ in trie)\n",
        "        return trie\n",
        "\n",
        "    @staticmethod\n",
        "    def _add_to_trie(sequence: List[int], trie_dict: Dict):\n",
        "        if sequence:\n",
        "            if sequence[0] not in trie_dict:\n",
        "                trie_dict[sequence[0]] = {}\n",
        "            Trie._add_to_trie(sequence[1:], trie_dict[sequence[0]])\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_from_trie(\n",
        "        prefix_sequence: List[int],\n",
        "        trie_dict: Dict,\n",
        "        append_trie=None,\n",
        "        bos_token_id: int = None,\n",
        "    ):\n",
        "        #tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\", max_length=1024)\n",
        "        #print (prefix_sequence)\n",
        "        #print (tokenizer.decode(prefix_sequence))\n",
        "        if len(prefix_sequence) == 0:\n",
        "            #print ('Prefix sequence is empty')\n",
        "            output = list(trie_dict.keys())\n",
        "            #if len(output)<100:\n",
        "             #   print ('Output: ',output)\n",
        "            if append_trie and bos_token_id in output:\n",
        "                output.remove(bos_token_id)\n",
        "                output += list(append_trie.trie_dict.keys())\n",
        "            return output\n",
        "\n",
        "        elif prefix_sequence[0] in trie_dict:\n",
        "\n",
        "            return Trie._get_from_trie(\n",
        "                prefix_sequence[1:],\n",
        "                trie_dict[prefix_sequence[0]],\n",
        "                append_trie,\n",
        "                bos_token_id,\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            #print ('Missing: ',prefix_sequence[0])\n",
        "            if append_trie:\n",
        "                return append_trie.get(prefix_sequence)\n",
        "            else:\n",
        "                return [2]\n",
        "\n",
        "    def __iter__(self):\n",
        "        def _traverse(prefix_sequence, trie_dict):\n",
        "            if trie_dict:\n",
        "                for next_token in trie_dict:\n",
        "                    yield from _traverse(\n",
        "                        prefix_sequence + [next_token], trie_dict[next_token]\n",
        "                    )\n",
        "            else:\n",
        "                yield prefix_sequence\n",
        "\n",
        "        return _traverse([], self.trie_dict)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, value):\n",
        "        return self.get(value)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZkMhT77lLn0"
      },
      "source": [
        "Class that is child of DataSet class - Used to create training, dev, and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HbGBofclH9H"
      },
      "outputs": [],
      "source": [
        "# Create Custom DataSet\n",
        "\n",
        "# data processing class/DataSet\n",
        "\n",
        "class ChemDiseaseDataset(Dataset):\n",
        "\n",
        "    def __init__(self,encodings,labels,test_set=False):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.test_set = test_set\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "\n",
        "        item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
        "        item['label_ids'] = torch.tensor(self.labels['labels'][index])\n",
        "        item['decoder_input_ids'] = torch.tensor(self.labels['decoder_input_ids'][index])   #I COMMENTED THIS FOR THE LABEL ONLY TRAINING WIHTOUT DECODER INPUT ID.\n",
        "        item['decoder_attention_mask'] = torch.tensor(self.labels['attention_mask'][index])\n",
        "        # the decoder atten mask has the same length as label of decoder input\n",
        "        if self.test_set:\n",
        "            item['decoder_input_ids_test'] = torch.tensor(self.labels['decoder_input_ids_test'][index])\n",
        "            item['decoder_attention_mask_test'] = torch.tensor(self.labels['attention_mask_test'][index])\n",
        "\n",
        "\n",
        "        return item\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels['labels'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def return_target_tokens(file_path,tokenizer,prefix_mention_is):\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        output =[line.strip('\\n') for line in f.readlines()]\n",
        "\n",
        "    #output=output[200:205]\n",
        "    tokens_y = {'labels':[], 'attention_mask':[], 'decoder_input_ids':[], 'decoder_input_ids_test':[], 'attention_mask_test':[], 'unlikelihood_tokens':[]}\n",
        "\n",
        "    max_len_y=0\n",
        "    for item in tqdm(output):\n",
        "\n",
        "        iy = json.loads(item)\n",
        "        prefix = list(tokenizer(' '+iy[0])['input_ids'])[1:-1]\n",
        "        label = list(tokenizer(' '+iy[1])['input_ids'])[1:-1]\n",
        "        y =prefix+label\n",
        "        max_len_y = np.max([max_len_y, len(y)+2])\n",
        "\n",
        "        if prefix_mention_is:\n",
        "            tokens_y['decoder_input_ids'].append([2] + y) #decoder input is the entire decoder with the SEP token\n",
        "            labs_prefix = [-100] * len(prefix) + label + [2]\n",
        "            tokens_y['labels'].append(labs_prefix) #masking the prompt part of the label.\n",
        "            assert len(labs_prefix) == len(y) + 1\n",
        "        else:\n",
        "            tokens_y['decoder_input_ids'].append([2] + label) #decoder input\n",
        "            tokens_y['labels'].append(label + [2]) # labels\n",
        "\n",
        "        tokens_y['attention_mask'].append(list(np.ones_like(tokens_y['decoder_input_ids'][-1])))\n",
        "\n",
        "        #in training we provide the entire sentence but in test we do not.\n",
        "        if 'test' in file_path:\n",
        "            #print (\"For the test section only. Not for dev section.\")\n",
        "            if prefix_mention_is:\n",
        "                tokens_y['decoder_input_ids_test'].append(prefix) #I ADDED THE 2 HERE.\n",
        "            else:\n",
        "                tokens_y['decoder_input_ids_test'].append([2]) #is 2 correct here for BOS or is it separator.\n",
        "            tokens_y['attention_mask_test'].append(list(np.ones_like(tokens_y['decoder_input_ids_test'][-1])))\n",
        "\n",
        "    #note attention_mask_test and decoder_input_ids_test is not padded. do we need to do it?\n",
        "    for index in range(len(tokens_y['decoder_input_ids'])):\n",
        "         tokens_y['decoder_input_ids'][index] = list(np.pad(tokens_y['decoder_input_ids'][index], ((0,max_len_y- len(tokens_y['decoder_input_ids'][index]))), 'constant', constant_values = (1,1)))\n",
        "\n",
        "    for index in range(len(tokens_y['attention_mask'])):\n",
        "         tokens_y['attention_mask'][index] = list(np.pad(tokens_y['attention_mask'][index], ((0,max_len_y- len(tokens_y['attention_mask'][index]))), 'constant', constant_values = (0,0)))\n",
        "\n",
        "    for index in range(len(tokens_y['labels'])):\n",
        "         tokens_y['labels'][index] = np.pad(tokens_y['labels'][index], ((0,max_len_y - len(tokens_y['labels'][index]))), 'constant', constant_values = (-100,-100))\n",
        "         tokens_y['labels'][index] = list(tokens_y['labels'][index].astype(np.int64))\n",
        "\n",
        "\n",
        "    return tokens_y\n",
        "\n",
        "\n",
        "def return_source_tokens(file_path,tokenizer):\n",
        "    with open(file_path, 'r') as f:\n",
        "        output =[line.strip('\\n') for line in f.readlines()]\n",
        "\n",
        "    #output =output[200:205]\n",
        "    token_x={'input_ids':[], 'attention_mask':[]}\n",
        "\n",
        "    for x in tqdm(output):\n",
        "        ix = json.loads(x)[0]\n",
        "        line = tokenizer(' '+ix,padding='max_length',truncation=True)\n",
        "        token_x['input_ids'].append(line['input_ids'])\n",
        "        token_x['attention_mask'].append(line['attention_mask'])\n",
        "\n",
        "    return token_x\n",
        "\n",
        "\n",
        "\n",
        "def create_pretrained_datasets(tokenizer,dataset_path,prefix_mention_is=False,evaluate=False):\n",
        "     if evaluate:\n",
        "                file_path = f'{dataset_path}/test'\n",
        "                test_token_x = return_source_tokens(file_path+'.source',tokenizer)\n",
        "                test_token_y = return_target_tokens(file_path+'.target',tokenizer,prefix_mention_is)\n",
        "                test_set = ChemDiseaseDataset(test_token_x, test_token_y,test_set=True)\n",
        "\n",
        "                return None,None,test_set\n",
        "     else:\n",
        "                file_path = f'{dataset_path}/train'\n",
        "                train_token_x = return_source_tokens(file_path+'.source',tokenizer)\n",
        "                train_token_y = return_target_tokens(file_path+'.target',tokenizer,prefix_mention_is)\n",
        "                train_set = ChemDiseaseDataset(train_token_x, train_token_y,test_set=False)\n",
        "\n",
        "                file_path = f'{dataset_path}/dev'\n",
        "                dev_token_x = return_source_tokens(file_path+'.source',tokenizer)\n",
        "                dev_token_y = return_target_tokens(file_path+'.target',tokenizer,prefix_mention_is)\n",
        "                dev_set = ChemDiseaseDataset(dev_token_x, dev_token_y,test_set=False)\n",
        "                return train_set,dev_set,None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG1WHrYclYC8"
      },
      "source": [
        "Training Method - Used for FineTuning - after pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok-JWEDq1no6"
      },
      "outputs": [],
      "source": [
        "#Training function\n",
        "\n",
        "def train(prefix_mention_is,evaluation):\n",
        "    model_load_path = './pre_train_model/'   #pretrained model\n",
        "    max_position_embeddings = 1024\n",
        "    attention_dropout = 0.1\n",
        "    dropout = 0.1\n",
        "    dataset_path = './data/bc5cdr/'     #data path containing sample test and dev data\n",
        "    model_save_path = './model_checkpoints/'  #path to save finetuned model\n",
        "    model_token_path = 'facebook/bart-large'\n",
        "\n",
        "    output_dir= './model_checkpoints/'\n",
        "    num_train_epochs= 5\n",
        "    per_device_train_batch_size= 16\n",
        "    per_device_eval_batch_size=1\n",
        "    warmup_steps=2000\n",
        "    weight_decay=0.01\n",
        "    logging_path= './logs/'\n",
        "    logging_steps=1000\n",
        "    save_steps=5000\n",
        "    evaluation_strategy='steps'\n",
        "    init_lr=1e-5 #1e-5 is original\n",
        "    label_smoothing_factor=0.1\n",
        "    max_grad_norm=0.1\n",
        "    max_steps=20000\n",
        "    eval_steps=120\n",
        "    lr_scheduler_type= 'polynomial'\n",
        "    seed=71\n",
        "    gradient_accumulation_steps=1 #50\n",
        "    #lr_scheduler_kwargs = {'lr_end':1e-8,'power':1.0}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    bartconf = BartConfig.from_pretrained(\"facebook/bart-large\")\n",
        "    #bartconf = BartConfig.from_pretrained(model_load_path)\n",
        "    bartconf.max_position_embeddings = max_position_embeddings\n",
        "    bartconf.attention_dropout = attention_dropout\n",
        "    bartconf.dropout = dropout\n",
        "\n",
        "\n",
        "    print (dataset_path)\n",
        "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\", max_length=1024)\n",
        "\n",
        "    #model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", config = bartconf)\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_load_path, config = bartconf)  #use pretrained model.\n",
        "\n",
        "    ## REMEMBER TO CHANGE THE LOAD PATH BACK FROM CHECKPOINT\n",
        "    #model = BartForConditionalGeneration.from_pretrained(model_save_path+'checkpoint-10000/', config = bartconf)  #use appropriate path for loading from checkpoint\n",
        "    model.to('cpu')\n",
        "\n",
        "    train_dataset, eval_dataset, _ = create_pretrained_datasets(tokenizer,\n",
        "                                                    dataset_path,\n",
        "                                                    prefix_mention_is = prefix_mention_is,\n",
        "                                                    evaluate = evaluation,\n",
        "                                                    )\n",
        "    #train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "    print ('dataset loaded')\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "                output_dir=output_dir,          # output directory\n",
        "                num_train_epochs=num_train_epochs,              # total number of training epochs\n",
        "                per_device_train_batch_size=per_device_train_batch_size,  # batch size per device during training\n",
        "                per_device_eval_batch_size=per_device_eval_batch_size,   # batch size for evaluation\n",
        "                warmup_steps=warmup_steps,                # number of warmup steps for learning rate scheduler\n",
        "                weight_decay=weight_decay,               # strength of weight decay\n",
        "                logging_dir=logging_path,            # directory for storing logs\n",
        "                logging_steps=logging_steps,\n",
        "                save_steps=save_steps,\n",
        "                evaluation_strategy=evaluation_strategy,\n",
        "                eval_steps=eval_steps,\n",
        "                learning_rate=init_lr,\n",
        "                label_smoothing_factor=label_smoothing_factor,\n",
        "                max_grad_norm=max_grad_norm,\n",
        "                max_steps=max_steps,\n",
        "                lr_scheduler_type=lr_scheduler_type,\n",
        "                seed=seed,\n",
        "                gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "                #lr_scheduler_kwargs =  lr_scheduler_kwargs   #custom arguments for polynomial LR scheduler\n",
        "                #dropout=dropout,\n",
        "                #attention_dropout=dropout\n",
        "                )\n",
        "\n",
        "\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset\n",
        "    )\n",
        "\n",
        "    #finetuning\n",
        "    trainer.train()\n",
        "\n",
        "    #save fine tuned model\n",
        "\n",
        "    #trainer.save_model(model_save_path)\n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWSbCw3NXU3l"
      },
      "outputs": [],
      "source": [
        "# Evaluation Function\n",
        "\n",
        "def evaluate(prefix_mention_is,evaluation,testset):\n",
        "\n",
        "    model_load_path = './pre_train_model/'   #pre trained model\n",
        "    max_position_embeddings = 1024\n",
        "    attention_dropout = 0 #0.1\n",
        "    dropout = 0 #0.1\n",
        "    dataset_path = './data/bc5cdr/'\n",
        "    model_save_path = './model_checkpoints/'   #saved finetuned model\n",
        "    model_token_path = \"facebook/bart-large\"   #HF model tokenizer\n",
        "    trie_path = './data/bc5cdr/trie.pkl'  #prefix tree\n",
        "    max_length=2014\n",
        "\n",
        "\n",
        "\n",
        "    print ('Evaluation')\n",
        "    print(torch.cuda.is_available())\n",
        "    bartconf = BartConfig.from_pretrained(model_load_path)\n",
        "    bartconf.max_position_embeddings = max_position_embeddings\n",
        "    bartconf.attention_dropout = attention_dropout\n",
        "    bartconf.dropout = dropout\n",
        "    bartconf.max_length = max_length\n",
        "\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_token_path)\n",
        "    #model = BartForConditionalGeneration.from_pretrained(model_load_path, config = bartconf)\n",
        "\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_save_path+'checkpoint-15000/', config = bartconf)  #loading saved model after finetuning\n",
        "\n",
        "    #model = model.cuda().to(model.device)\n",
        "    model.to('cpu')\n",
        "    print (model.device)\n",
        "\n",
        "    _, dev_dataset, test_dataset = create_pretrained_datasets(tokenizer,\n",
        "                                                    dataset_path,\n",
        "                                                    prefix_mention_is = prefix_mention_is,\n",
        "                                                    evaluate = evaluation)\n",
        "\n",
        "    if testset:\n",
        "        print('eval on test set')\n",
        "        eval_dataset = test_dataset\n",
        "    else:\n",
        "        print('eval on develop set')\n",
        "        eval_dataset = dev_dataset\n",
        "\n",
        "\n",
        "    #loading the prefix tree\n",
        "    print('loading trie......')\n",
        "    with open(trie_path, \"rb\") as f:\n",
        "        trie = Trie.load_from_dict(pickle.load(f))\n",
        "    print('trie loaded.......')\n",
        "\n",
        "    #will have to use custom Trie class methods\n",
        "\n",
        "    bad_words=['is','to',' '+tokenizer.unk_token,' '+'to',tokenizer.unk_token]\n",
        "    bad_word_ids = tokenizer(bad_words, add_prefix_space=True, add_special_tokens=False).input_ids\n",
        "\n",
        "    top5results=[]\n",
        "    for i,example in enumerate(eval_dataset):\n",
        "        print('Example: ',i)\n",
        "        example_result={}\n",
        "        input_ids = example['input_ids'].unsqueeze(0)\n",
        "        print (\"Decoder prompt: \",tokenizer.decode(example['decoder_input_ids_test'], skip_special_tokens=True))\n",
        "        encoder_attention_mask = example['attention_mask'].unsqueeze(0)\n",
        "        decoder_input_ids = example['decoder_input_ids_test'][:-1].unsqueeze(0)  #FOR WITH PROMPT\n",
        "        #decoder_input_ids = example['decoder_input_ids_test'].unsqueeze(0)  #FOR WITHOUT PROMPT\n",
        "        decoder_attention_mask = example['decoder_attention_mask_test'].unsqueeze(0)\n",
        "        input_ids = input_ids.to(model.device)\n",
        "        encoder_attention_mask = encoder_attention_mask.to(model.device)\n",
        "        decoder_input_ids = decoder_input_ids.to(model.device)\n",
        "        decoder_attention_mask = decoder_attention_mask.to(model.device)\n",
        "\n",
        "        beam_output = model.generate(input_ids,\n",
        "                                     bad_words_ids=bad_word_ids,\n",
        "                                     decoder_input_ids=decoder_input_ids,\n",
        "                                     attention_mask=encoder_attention_mask,\n",
        "                                     decoder_attention_mask=decoder_attention_mask,\n",
        "                                     #max_length=10,\n",
        "                                     max_new_tokens=20,\n",
        "                                     num_beams=10,\n",
        "                                     #do_sample=True,  #uncomment to run sampling beam search\n",
        "                                     #top_k=500,   #uncomment to run sampling beam search\n",
        "                                     num_return_sequences=5,\n",
        "                                     decoder_start_token_id=2,\n",
        "                                     early_stopping=True,\n",
        "                                     ##### all hyperparams below are set to default\n",
        "                                     #temperature=5.0, #uncomment to run sampling beam search\n",
        "                                     repetition_penalty=1.2, #1.2\n",
        "                                     no_repeat_ngram_size=0,\n",
        "                                     length_penalty=0.1,\n",
        "                                     remove_invalid_values=True,\n",
        "                                     prefix_allowed_tokens_fn=lambda batch_id, sent: custom_constraint(trie,sent),\n",
        "                                    )\n",
        "\n",
        "\n",
        "        generated_text1 = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "        print ('text1 :',generated_text1)\n",
        "\n",
        "        generated_text2 = tokenizer.decode(beam_output[1], skip_special_tokens=True)\n",
        "        print ('text2 :',generated_text2)\n",
        "        generated_text3 = tokenizer.decode(beam_output[2], skip_special_tokens=True)\n",
        "        print ('text3 :',generated_text3)\n",
        "        generated_text4 = tokenizer.decode(beam_output[3], skip_special_tokens=True)\n",
        "        print ('text4 :',generated_text4)\n",
        "        generated_text5 = tokenizer.decode(beam_output[4], skip_special_tokens=True)\n",
        "        print ('text5 :',generated_text5)\n",
        "\n",
        "        example_result['text1']=generated_text1.strip()\n",
        "        example_result['text2']=generated_text2.strip()\n",
        "        example_result['text3']=generated_text3.strip()\n",
        "        example_result['text4']=generated_text4.strip()\n",
        "        example_result['text5']=generated_text5.strip()\n",
        "        top5results.append(example_result)\n",
        "        top5_df = pd.DataFrame(top5results,columns=['text1','text2','text3','text4','text5'])\n",
        "        if testset:\n",
        "            top5_df.to_csv('Test_nb10_t5_rp1.2_pmtTrue_sampleTrue_topk50000_lp0.1.csv')\n",
        "        else:\n",
        "            top5_df.to_csv('Dev_nb10_t5_rp1.2_pmtTrue_sampleTrue_topk50000_lp0.1.csv')\n",
        "\n",
        "    return top5results\n",
        "\n",
        "def custom_constraint(trie,sequence):\n",
        "    #print (\"Existing :\",sequence)\n",
        "    return trie.get(sequence.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anZyFRWGXcEj"
      },
      "outputs": [],
      "source": [
        "# Results Function\n",
        "def find_cui_str(dict_path):\n",
        "    #loading the json file with MeSH concept and synonyms.\n",
        "    #concept with the list of synonyms\n",
        "\n",
        "    with open(dict_path, 'r') as f:\n",
        "        cui2str = json.load(f)\n",
        "\n",
        "    #cui:[list of strings]\n",
        "    return cui2str\n",
        "\n",
        "\n",
        "def find_str_cui(cui2str):\n",
        "     #mapping the synonyms to concept\n",
        "    str2cui = {}\n",
        "    for cui in cui2str:\n",
        "        if isinstance(cui2str[cui], list):\n",
        "            for name in cui2str[cui]:\n",
        "                if name in str2cui:\n",
        "                    str2cui[name].append(cui)\n",
        "                else:\n",
        "                    str2cui[name] = [cui]\n",
        "        else:\n",
        "            name = cui2str[cui]\n",
        "            if name in str2cui:\n",
        "                str2cui[name].append(cui)\n",
        "                print('duplicated vocabulary')\n",
        "            else:\n",
        "                str2cui[name] = [cui]\n",
        "    print('dictionary loaded......')\n",
        "    #dict with str:list of cui..mostly one cui\n",
        "\n",
        "    return str2cui\n",
        "\n",
        "def find_true_labels(testset,dataset_path):\n",
        "    if testset:\n",
        "        #loading test label cuis\n",
        "        print('loading label cuis......')\n",
        "        with open(dataset_path+'/testlabel.txt', 'r') as f:\n",
        "            cui_labels = [set(cui.strip('\\n').replace('+', '|').split('|')) for cui in f.readlines()]\n",
        "            cui_labels=[list(item) for item in cui_labels]\n",
        "        print('label cuis loaded')\n",
        "        return cui_labels\n",
        "\n",
        "    else:\n",
        "\n",
        "        #loading dev label cuis\n",
        "        print('loading dev label cuis......')\n",
        "        with open(dataset_path+'/devlabel.txt', 'r') as f:\n",
        "            dev_cui_labels = [set(cui.strip('\\n').replace('+', '|').split('|')) for cui in f.readlines()]\n",
        "            dev_cui_labels=[list(item) for item in dev_cui_labels]\n",
        "        print('dev label cuis loaded')\n",
        "        return dev_cui_labels\n",
        "\n",
        "\n",
        "def find_precision(top5results,true_labels,str2cui,testset):\n",
        "    precision1,precision3,precision5=[],[],[]\n",
        "    cui_dict={'cui_text1':[],'cui_text2':[],'cui_text3':[],'cui_text4':[],'cui_text5':[],'ground_truth':[item[0] for item in true_labels]}\n",
        "    for i in range(len(top5results)):\n",
        "        truth = true_labels[i][0]\n",
        "        result_dict = top5results[i]\n",
        "\n",
        "        p1=0  #Precision@1\n",
        "        p3=0  #Precision@3\n",
        "        p5=0  #Precision@5\n",
        "        count=0\n",
        "        for k,v in result_dict.items():\n",
        "\n",
        "           cui_dict['cui_'+k].append(str2cui.get(v,[None])[0])\n",
        "           if truth in str2cui.get(v,[None]):\n",
        "                if count==0:\n",
        "                   p1+=1\n",
        "                   p3+=1\n",
        "                   p5+=1\n",
        "                elif count>0 and count<3:\n",
        "                    p3+=1\n",
        "                    p5+=1\n",
        "                else:\n",
        "                    p5+=1\n",
        "           count+=1\n",
        "\n",
        "\n",
        "\n",
        "        precision1.append(float(p1/1))\n",
        "        precision3.append(float(p3/3))\n",
        "        precision5.append(float(p5/5))\n",
        "\n",
        "    print (precision5)\n",
        "    avg_top1_precision= np.mean(precision1)\n",
        "    avg_top3_precision= np.mean(precision3)\n",
        "    avg_top5_precision= np.mean(precision5)\n",
        "\n",
        "    cui_df = pd.DataFrame(cui_dict,columns=list(cui_dict.keys()))\n",
        "\n",
        "    if testset:\n",
        "        cui_df.to_csv('TEST_Top5outputswithprompt.csv')\n",
        "\n",
        "        print ('TEST_Average TOP 1 Precision: ',round(avg_top1_precision,2))\n",
        "        print ('TEST_Average TOP 3 Precision: ',round(avg_top3_precision,2))\n",
        "        print ('TEST_Average TOP 5 Precision: ',round(avg_top5_precision,2))\n",
        "    else:\n",
        "        cui_df.to_csv('DEV_Top5outputswithprompt.csv')\n",
        "\n",
        "        print ('DEV_Average Top 1 Precision: ',round(avg_top1_precision,2))\n",
        "        print ('DEV_Average TOP 3 Precision: ',round(avg_top3_precision,2))\n",
        "        print ('DEV_Average TOP 5 Precision: ',round(avg_top5_precision,2))\n",
        "\n",
        "\n",
        "    return precision1,precision3,precision5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyQo8csybGdj",
        "outputId": "5dec0d69-36c4-423e-a07b-422a7bfddade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation\n",
            "True\n",
            "cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 448.66it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 3633.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval on test set\n",
            "loading trie......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trie loaded.......\n",
            "Example:  0\n",
            "Decoder prompt:   malondialdehyde is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text1 :  malondialdehyde\n",
            "text2 :  malondialdehyde sodium\n",
            "text3 :  malondialdehyde low density lipoprotein human\n",
            "text4 :  malondialdehyde low density lipoprotein mouse\n",
            "text5 :  malondialdehyde low density lipoprotein human\n",
            "Example:  1\n",
            "Decoder prompt:   nitric oxide is\n",
            "text1 :  nitric oxide\n",
            "text2 :  nitric oxide synthase\n",
            "text3 :  nitric oxide reductase\n",
            "text4 :  nitric oxide receptors\n",
            "text5 :  nitric oxide synthetase\n",
            "Example:  2\n",
            "Decoder prompt:   glutathione is\n",
            "text1 :  glutathione synthase\n",
            "text2 :  glutathione oxidase\n",
            "text3 :  glutathione oxidized\n",
            "text4 :  glutathione sulfonate\n",
            "text5 :  glutathione sulfonamide\n",
            "Example:  3\n",
            "Decoder prompt:   superoxide is\n",
            "text1 :  superoxide anion\n",
            "text2 :  superoxide radical\n",
            "text3 :  superoxide\n",
            "text4 :  superoxide reductase\n",
            "text5 :  superoxide dismutase\n",
            "Example:  4\n",
            "Decoder prompt:   isoproterenol is\n",
            "text1 :  isoproterenol\n",
            "text2 :  isoproterenol hydrochloride\n",
            "text3 :  isoproterenol sulfate\n",
            "text4 :  isoproterenol bitartrate\n",
            "text5 :  isoproterenol bitartrate\n",
            "Example:  5\n",
            "Decoder prompt:   captopril is\n",
            "text1 :  captopril\n",
            "text2 :  captopril cysteine\n",
            "text3 :  captopril ethyl ester\n",
            "text4 :  captopril l cysteine\n",
            "text5 :  captopril disulfide\n",
            "Example:  6\n",
            "Decoder prompt:   angiotensin is\n",
            "text1 :  angiotensin a\n",
            "text2 :  angiotensin\n",
            "text3 :  angiotensin i\n",
            "text4 :  angiotensin ii\n",
            "text5 :  angiotensin amide\n",
            "Example:  7\n",
            "Decoder prompt:   malondialdehyde is\n",
            "text1 :  malondialdehyde\n",
            "text2 :  malondialdehyde sodium\n",
            "text3 :  malondialdehyde low density lipoprotein human\n",
            "text4 :  malondialdehyde low density lipoprotein mouse\n",
            "text5 :  malondialdehyde low density lipoprotein human\n",
            "Example:  8\n",
            "Decoder prompt:   isoproterenol is\n",
            "text1 :  isoproterenol\n",
            "text2 :  isoproterenol hydrochloride\n",
            "text3 :  isoproterenol bitartrate\n",
            "text4 :  isoproterenol sulfate\n",
            "text5 :  isoproterenol bitartrate\n",
            "Example:  9\n",
            "Decoder prompt:   lenalidomide is\n",
            "text1 :  lenalidomide\n",
            "text2 :  lenalidomide\n",
            "text3 :  lenalidomide\n",
            "text4 :  lenalidomide\n",
            "text5 :  lenalidomide\n",
            "Example:  10\n",
            "Decoder prompt:   dexamethasone is\n",
            "text1 :  dexamethasone\n",
            "text2 :  dexamethasone glucuronide\n",
            "text3 :  dexamethasone valerate\n",
            "text4 :  dexamethasone beta d glucuronide\n",
            "text5 :  dexamethasone dipropionate\n",
            "Example:  11\n",
            "Decoder prompt:   multiple myeloma is\n",
            "text1 :  multiple myeloma\n",
            "text2 :  multiple myeloma m proteins\n",
            "text3 :  multiple myeloma protein 2 human\n",
            "text4 :  multiple myeloma protein 2 human\n",
            "text5 :  multiple myeloma protein 2 human\n",
            "Example:  12\n",
            "Decoder prompt:   myeloma is\n",
            "text1 :  myeloma multiples\n",
            "text2 :  myeloma multiple\n",
            "text3 :  myeloma plasma cell\n",
            "text4 :  myeloma immunoglobulins\n",
            "text5 :  myeloma cell activator\n",
            "Example:  13\n",
            "Decoder prompt:   lenalidomide is\n",
            "text1 :  lenalidomide\n",
            "text2 :  lenalidomide\n",
            "text3 :  lenalidomide\n",
            "text4 :  lenalidomide\n",
            "text5 :  lenalidomide\n",
            "Example:  14\n",
            "Decoder prompt:   dexamethasone is\n",
            "text1 :  dexamethasone\n",
            "text2 :  dexamethasone glucuronide\n",
            "text3 :  dexamethasone valerate\n",
            "text4 :  dexamethasone dipropionate\n",
            "text5 :  dexamethasone oxetanone\n",
            "Example:  15\n",
            "Decoder prompt:   multiple myeloma is\n",
            "text1 :  multiple myeloma\n",
            "text2 :  multiple myeloma m proteins\n",
            "text3 :  multiple myeloma protein 2 human\n",
            "text4 :  multiple myeloma protein 2 human\n",
            "text5 :  multiple myeloma protein 2 human\n",
            "Example:  16\n",
            "Decoder prompt:   relapsed/refractory multiple myeloma is\n",
            "text1 :  relapsed/refractory multiple myeloma\n",
            "text2 :  relapsed/refractory multiple myeloma\n",
            "text3 :  relapsed/refractory multiple myeloma\n",
            "text4 :  relapsed/refractory multiple myeloma\n",
            "text5 :  relapsed/refractory multiple myeloma\n",
            "Example:  17\n",
            "Decoder prompt:   relapsed/refractory multiple myeloma is\n",
            "text1 :  relapsed/refractory multiple myeloma\n",
            "text2 :  relapsed/refractory multiple myeloma\n",
            "text3 :  relapsed/refractory multiple myeloma\n",
            "text4 :  relapsed/refractory multiple myeloma\n",
            "text5 :  relapsed/refractory multiple myeloma\n",
            "Example:  18\n",
            "Decoder prompt:   thalidomide is\n",
            "text1 :  thalidomide\n",
            "text2 :  thalidomide\n",
            "text3 :  thalidomide\n",
            "text4 :  thalidomide\n",
            "text5 :  thalidomide\n",
            "Example:  19\n",
            "Decoder prompt:   myelosuppression is\n",
            "text1 :  myelosuppression\n",
            "text2 :  myelosuppression\n",
            "text3 :  myelosuppression\n",
            "text4 :  myelosuppression\n",
            "text5 :  myelosuppression\n",
            "Example:  20\n",
            "Decoder prompt:   peripheral neuropathy is\n",
            "text1 :  peripheral neuropathy\n",
            "text2 :  peripheral neuropathy alcohol induced\n",
            "text3 :  peripheral neuropathy alcohol ind\n",
            "text4 :  peripheral neuropathy paraneopl\n",
            "text5 :  peripheral neuropathy paraneoplastic\n",
            "Example:  21\n",
            "Decoder prompt:   deep vein thrombosis is\n",
            "text1 :  deep vein thrombosis\n",
            "text2 :  deep vein thrombosis\n",
            "text3 :  deep vein thrombosis\n",
            "text4 :  deep vein thrombosis\n",
            "text5 :  deep vein thrombosis\n",
            "Example:  22\n",
            "Decoder prompt:   lenalidomide is\n",
            "text1 :  lenalidomide\n",
            "text2 :  lenalidomide\n",
            "text3 :  lenalidomide\n",
            "text4 :  lenalidomide\n",
            "text5 :  lenalidomide\n",
            "Example:  23\n",
            "Decoder prompt:   relapsed/refractory multiple myeloma is\n",
            "text1 :  relapsed/refractory multiple myeloma\n",
            "text2 :  relapsed/refractory multiple myeloma\n",
            "text3 :  relapsed/refractory multiple myeloma\n",
            "text4 :  relapsed/refractory multiple myeloma\n",
            "text5 :  relapsed/refractory multiple myeloma\n",
            "Example:  24\n",
            "Decoder prompt:   ifosfamide is\n",
            "text1 :  ifosfamide\n",
            "text2 :  ifosfamide mustard\n",
            "text3 :  ifosfamide bromide mustard\n",
            "text4 :  ifosfamide bromide mustard\n",
            "text5 :  ifosfamide bromide mustard\n",
            "Example:  25\n",
            "Decoder prompt:   mesna is\n",
            "text1 :  mesna\n",
            "text2 :  mesna albumin\n",
            "text3 :  mesna cell\n",
            "text4 :  mesna albumin\n",
            "text5 :  mesna albumin\n",
            "Example:  26\n",
            "Decoder prompt:   ifosfamide is\n",
            "text1 :  ifosfamide\n",
            "text2 :  ifosfamide mustard\n",
            "text3 :  ifosfamide bromide mustard\n",
            "text4 :  ifosfamide bromide mustard\n",
            "text5 :  ifosfamide bromide mustard\n",
            "Example:  27\n",
            "Decoder prompt:   ifosfamide is\n",
            "text1 :  ifosfamide\n",
            "text2 :  ifosfamide mustard\n",
            "text3 :  ifosfamide bromide mustard\n",
            "text4 :  ifosfamide bromide mustard\n",
            "text5 :  ifosfamide bromide mustard\n",
            "Example:  28\n",
            "Decoder prompt:   nitrogen is\n",
            "text1 :  nitrogen compounds\n",
            "text2 :  nitrogen mustard cpds\n",
            "text3 :  nitrogen mustard compounds\n",
            "text4 :  nitrogen isotopes\n",
            "text5 :  nitrogen fixation\n",
            "Example:  29\n",
            "Decoder prompt:   hemorrhagic is\n",
            "text1 :  hemorrhagic dis\n",
            "text2 :  hemorrhagic shock\n",
            "text3 :  hemorrhagic septicemia\n",
            "text4 :  hemorrhagic septicaemia\n",
            "text5 :  hemorrhagic fever korean\n",
            "Example:  30\n",
            "Decoder prompt:   cystitis is\n",
            "text1 :  cystitis interstitial\n",
            "text2 :  cystitis\n",
            "text3 :  cystitis chronic interstitial\n",
            "text4 :  cystitis chronic interstitial\n",
            "text5 :  cystitis chronic interstitial\n",
            "Example:  31\n",
            "Decoder prompt:   ifosfamide is\n",
            "text1 :  ifosfamide\n",
            "text2 :  ifosfamide mustard\n",
            "text3 :  ifosfamide bromide mustard\n",
            "text4 :  ifosfamide bromide mustard\n",
            "text5 :  ifosfamide bromide mustard\n",
            "Example:  32\n",
            "Decoder prompt:   sodium 2-sulfanylethanesulfonate is\n",
            "text1 :  sodium 2-sulfanylethanesulfonate\n",
            "text2 :  sodium 2-sulfanylethanesulfonate\n",
            "text3 :  sodium 2-sulfanylethanesulfonate\n",
            "text4 :  sodium 2-sulfanylethanesulfonate\n",
            "text5 :  sodium 2-sulfanylethanesulfonate\n",
            "Example:  33\n",
            "Decoder prompt:   mesna is\n",
            "text1 :  mesna\n",
            "text2 :  mesna albumin\n",
            "text3 :  mesna cell\n",
            "text4 :  mesna albumin\n",
            "text5 :  mesna albumin\n",
            "Example:  34\n",
            "Decoder prompt:   ifosfamide is\n",
            "text1 :  ifosfamide\n",
            "text2 :  ifosfamide mustard\n",
            "text3 :  ifosfamide bromide mustard\n",
            "text4 :  ifosfamide bromide mustard\n",
            "text5 :  ifosfamide bromide mustard\n",
            "Example:  35\n",
            "Decoder prompt:   mesna is\n",
            "text1 :  mesna\n",
            "text2 :  mesna albumin\n",
            "text3 :  mesna cell\n",
            "text4 :  mesna albumin\n",
            "text5 :  mesna albumin\n",
            "Example:  36\n",
            "Decoder prompt:   mesna is\n",
            "text1 :  mesna\n",
            "text2 :  mesna albumin\n",
            "text3 :  mesna cell\n",
            "text4 :  mesna albumin\n",
            "text5 :  mesna albumin\n",
            "Example:  37\n",
            "Decoder prompt:   ifosfamide is\n",
            "text1 :  ifosfamide\n",
            "text2 :  ifosfamide mustard\n",
            "text3 :  ifosfamide bromide mustard\n",
            "text4 :  ifosfamide bromide mustard\n",
            "text5 :  ifosfamide bromide mustard\n",
            "Example:  38\n",
            "Decoder prompt:   ifosfamide is\n",
            "text1 :  ifosfamide\n",
            "text2 :  ifosfamide mustard\n",
            "text3 :  ifosfamide bromide mustard\n",
            "text4 :  ifosfamide bromide mustard\n",
            "text5 :  ifosfamide bromide mustard\n",
            "Example:  39\n",
            "Decoder prompt:   mesna is\n",
            "text1 :  mesna\n",
            "text2 :  mesna albumin\n",
            "text3 :  mesna cell\n",
            "text4 :  mesna albumin\n",
            "text5 :  mesna albumin\n",
            "Example:  40\n",
            "Decoder prompt:   ifosfamide is\n",
            "text1 :  ifosfamide\n",
            "text2 :  ifosfamide mustard\n",
            "text3 :  ifosfamide bromide mustard\n",
            "text4 :  ifosfamide bromide mustard\n",
            "text5 :  ifosfamide bromide mustard\n",
            "Example:  41\n",
            "Decoder prompt:   mesna is\n",
            "text1 :  mesna\n",
            "text2 :  mesna albumin\n",
            "text3 :  mesna cell\n",
            "text4 :  mesna albumin\n",
            "text5 :  mesna albumin\n",
            "Example:  42\n",
            "Decoder prompt:   ifosfamide is\n",
            "text1 :  ifosfamide\n",
            "text2 :  ifosfamide mustard\n",
            "text3 :  ifosfamide bromide mustard\n",
            "text4 :  ifosfamide bromide mustard\n",
            "text5 :  ifosfamide bromide mustard\n",
            "Example:  43\n",
            "Decoder prompt:   genotoxicity is\n",
            "text1 :  genotoxicity tests\n",
            "text2 :  genotoxicity test\n",
            "text3 :  genotoxicity tests\n",
            "text4 :  genotoxicity tests\n",
            "text5 :  genotoxicity tests\n",
            "Example:  44\n",
            "Decoder prompt:   levodopa is\n",
            "text1 :  levodopa\n",
            "text2 :  levodopa receptors\n",
            "text3 :  levodopa receptor\n",
            "text4 :  levodopa ethylester\n",
            "text5 :  levodopa methyl ester\n",
            "Example:  45\n",
            "Decoder prompt:   dyskinesia is\n",
            "text1 :  dyskinesia medication ind\n",
            "text2 :  dyskinesia drug ind\n",
            "text3 :  dyskinesia syndrome\n",
            "text4 :  dyskinesia oral\n",
            "text5 :  dyskinesia medication induced\n",
            "Example:  46\n",
            "Decoder prompt:   parkinson's disease is\n",
            "text1 :  parkinson's disease\n",
            "text2 :  parkinson's disease\n",
            "text3 :  parkinson's disease\n",
            "text4 :  parkinson's disease\n",
            "text5 :  parkinson's disease\n",
            "Example:  47\n",
            "Decoder prompt:   levodopa is\n",
            "text1 :  levodopa\n",
            "text2 :  levodopa receptors\n",
            "text3 :  levodopa receptor\n",
            "text4 :  levodopa ethylester\n",
            "text5 :  levodopa methyl ester\n",
            "Example:  48\n",
            "Decoder prompt:   parkinson's disease is\n",
            "text1 :  parkinson's disease\n",
            "text2 :  parkinson's disease\n",
            "text3 :  parkinson's disease\n",
            "text4 :  parkinson's disease\n",
            "text5 :  parkinson's disease\n",
            "Example:  49\n",
            "Decoder prompt:   parkinson's disease is\n",
            "text1 :  parkinson's disease\n",
            "text2 :  parkinson's disease\n",
            "text3 :  parkinson's disease\n",
            "text4 :  parkinson's disease\n",
            "text5 :  parkinson's disease\n",
            "dictionary loaded......\n",
            "loading label cuis......\n",
            "label cuis loaded\n",
            "[0.4, 0.2, 0.0, 0.6, 0.6, 0.2, 0.2, 0.4, 0.6, 1.0, 0.2, 0.2, 0.6, 1.0, 0.2, 0.2, 0.0, 0.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.2, 0.4, 0.2, 0.2, 0.0, 0.0, 0.2, 0.2, 0.0, 0.4, 0.2, 0.4, 0.4, 0.2, 0.2, 0.4, 0.2, 0.4, 0.2, 0.0, 0.2, 0.6, 0.0, 0.2, 0.0, 0.0]\n",
            "TEST_Average TOP 1 Precision:  0.72\n",
            "TEST_Average TOP 3 Precision:  0.43\n",
            "TEST_Average TOP 5 Precision:  0.3\n"
          ]
        }
      ],
      "source": [
        "#Main Method\n",
        "evaluation = False  #change to True for test/dev\n",
        "testset = False  #change to True  for test\n",
        "prefix_mention_is = True\n",
        "dict_path = './data/bc5cdr/target_kb.json'\n",
        "data_path = './data/bc5cdr/'\n",
        "\n",
        "\n",
        "if evaluation:\n",
        "    top5results = evaluate(prefix_mention_is,evaluation,testset)\n",
        "    cui2str = find_cui_str(dict_path)\n",
        "    str2cui = find_str_cui(cui2str)\n",
        "    true_labels = find_true_labels(testset,data_path)\n",
        "    p1,p3,p5 = find_precision(top5results,true_labels,str2cui,testset)\n",
        "\n",
        "else:\n",
        "    train(prefix_mention_is,evaluation)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
